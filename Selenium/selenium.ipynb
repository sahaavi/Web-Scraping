{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selenium Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the reasons why we have to learn selenium is because it allows us to scrape JavaScript driven websites, with this we'll be able to script websites with dynamic content.  \n",
    "How to identify if a website was buit with JavaScript?\n",
    "> Inspect the webpage in browser  \n",
    "<img src=\"../images/inspect.png\"  width=\"60%\" height=\"30%\">  \n",
    "Then go to the settings  \n",
    "<img src=\"../images/settings.png\"  width=\"60%\" height=\"30%\">  \n",
    "Find the `Debugger` option in settings and turn on the `Disable JavaScript` option.  \n",
    "<img src=\"../images/disable_JS.png\"  width=\"60%\" height=\"30%\">  \n",
    "Now reload the webpage. If the webpage doesn't reload or does not contain all the data like before that means the website built with JavaScript.  \n",
    "<img src=\"../images/load_error.png\"  width=\"60%\" height=\"30%\">  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download ChromeDriver according to your Google Chrome browser version. [ChromeDriver](https://chromedriver.chromium.org/downloads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from webdriver_manager.chrome import ChromeDriverManager\n",
    "# setup chrome driver\n",
    "# driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "# website to scrape\n",
    "# website = \"https://www.adamchoi.co.uk/overs/detailed\"\n",
    "\n",
    "from selenium import webdriver\n",
    "website = \"https://www.adamchoi.co.uk/overs/detailed\" # website to scrape\n",
    "path = \"C:/chromedriver_win32/chromedriver.exe\" # path to chromedriver.exe\n",
    "driver = webdriver.Chrome(path) # setup chrome driver\n",
    "driver.get(website) # open website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit() # close chrome driver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find element by id  `driver.find_element_by_id('id')`  \n",
    "Find element by class name `driver.find_element_by_class_name('class_name')`  \n",
    "Find element by tag name `driver.find_element_by_tag_name('tag')`. This is frequently used when you want to locate multiple elements with the same tag name.   \n",
    "Find element by XPath `driver.find_element_by_xpath('//tag[@AttributeName=\"Value\"]')`  \n",
    "Find element by CSS Selector `driver.find_element_by_css_selector()`  \n",
    "Find element by name `driver.find_element_by_name()`  \n",
    "Find element by link text `driver.find_element_by_link_text()`  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to find elemnets instead of a single element, put a s after element in the function name.  \n",
    "For ex. `driver.find_elements_by_class_name('class_name')`  \n",
    "This will return a list of all the elements of given class_name whereas `find_element` will return only one element."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other things you can locate with selenium are dropdowns, also can choose any element listed in a dropdown. Also can `Login` to websites and scrape dynamic websites using `Waits`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Locate any element in Browser using XPath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find the .py file for this [here](https://github.com/sahaavi/Web-Scraping/blob/main/Selenium/argentina_matches.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape data from table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After going into the inspect option, use `ctrl+F` to open the XPath input field.  \n",
    "![xpath field in browser](../images/xpath.png)\n",
    "<!-- <img src=\"https://github.com/sahaavi/Web-Scraping/blob/main/images/xpath.png\"> -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "website = \"https://www.adamchoi.co.uk/overs/detailed\" # website to scrape\n",
    "path = \"C:/chromedriver_win32/chromedriver.exe\" # path to chromedriver.exe\n",
    "driver = webdriver.Chrome(path) # setup chrome driver\n",
    "driver.get(website) # open website\n",
    "\n",
    "# Add implicit wait\n",
    "driver.implicitly_wait(10)  # Wait up to 10 seconds for elements to be found\n",
    "\n",
    "# remember if you use single quotes inside for attribute value, use double quotes outside for the whole xpath or vice versa\n",
    "all_matches_button = driver.find_element_by_xpath(\"//label[@analytics-event='All matches']\") # find element by xpath\n",
    "all_matches_button.click() # click on the element\n",
    "\n",
    "matches = driver.find_elements_by_tag_name('tr') # find all elements by tr tag name \n",
    "\n",
    "for match in matches: # loop through all matches\n",
    "    print(match.text) # print text of each match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code will store each row information in one cell but we want to make them seperate. To get each column seperately inside a row we can use XPath.  \n",
    "![column](../images/column.png)\n",
    "<!-- <img src=\"https://github.com/sahaavi/Web-Scraping/blob/main/images/column.png\"> -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store scraped data in lists\n",
    "date = []\n",
    "home_team = []\n",
    "score = []\n",
    "away_team = []\n",
    "\n",
    "for match in matches: # loop through all matches\n",
    "    date.append(match.find_element_by_xpath('./td[1]').text)\n",
    "    home_team.append(match.find_element_by_xpath('./td[2]').text)\n",
    "    score.append(match.find_element_by_xpath('./td[3]').text)\n",
    "    away_team.append(match.find_element_by_xpath('./td[4]').text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit() # close chrome driver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Dropdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the dropdown is dynamic (built with AJAX), means when you choose different option from dropdown the website link won't change, only the data of page will change, then we have to use `Select()` function in selenium."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.support.ui import Select\n",
    "\n",
    "dropdown = Select(driver.find_element_by_id('country')) # find dropdown element by id\n",
    "dropdown.select_by_visible_text('Argentina') # select dropdown option by visible text\n",
    "\n",
    "# in the actual website when you change the dropdown option, the table is updated after a few seconds\n",
    "# so we need to wait for the table to be updated before we scrape the data\n",
    "# otherwise code can break if we try to scrape the data before it is updated\n",
    "# we can use time.sleep() to wait for a few seconds \n",
    "# but we can also use driver.implicitly_wait()\n",
    "import time\n",
    "time.sleep(5) # wait 5 seconds for elements to be"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the dropdown selection in action [here](https://github.com/sahaavi/Web-Scraping/blob/main/Selenium/argentina_matches.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check how to handle pagination with Selenium and built an Amazon Audible Bot [here](https://github.com/sahaavi/Web-Scraping/blob/main/Selenium/audible_bot.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Headless Mode` on Selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Headless mode is an option that Selenium has that let us run the bot we create on the background.  \n",
    "This is useful when you're doing other tasks and you don't want to see that browser opening over and over again every time selenium is scraping the website so you can let the machine do the job silently on the background."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the `Headless Audible Bot` in action [here](https://github.com/sahaavi/Web-Scraping/blob/main/Selenium/headless_audible_bot.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pagination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the `Selenium Pagination with Audible Bot` in action [here](https://github.com/sahaavi/Web-Scraping/blob/main/Selenium/selenium_pagination.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implicit vs Explicit Waits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implicit wait is used to tell the driver to wait for a certain amount of time when trying to locate an element. As you have seen in the previous example we use `sleep()` function of `time` library and the `driver.implicitly_wait(10)` to make the implicit wait. Implicit wait make the driver wait for the given amount of time to make sure all the data of page has been loaded.  \n",
    "On the other hand explicit wait makes the web driver wait for a specific condition to occur before proceeding further with the execution.\n",
    "To work with the explicit wait we have import 3 libraries- `By`, `WebDriverWait`, `EC`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "WebDriverWait(driver, 10).until(....) # wait until this condition is met for atmost 10 seconds\n",
    "# after 10 seconds if condition is not met, it will break the code\n",
    "# choosing the number of seconds depend on the website as well as the local machine \n",
    "# Inside until the write the condition as following\n",
    "EC.presence_of_element_located(..) # wait until the presence of element is located, otherwise the code will break\n",
    "# There are different types of conditions you can use\n",
    "# https://selenium-python.readthedocs.io/waits.html\n",
    "EC.element_to_be_clickable(..) \n",
    "EC.element_to_be_selected(..)\n",
    "# the last part is how to locate an element such as xpath, id, class name etc\n",
    "(By.XPATH, \"Value\")\n",
    "# the whole code will look like this\n",
    "WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.XPATH, \"Value\")))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the `Explicit wait with Audible Bot` in action [here](https://github.com/sahaavi/Web-Scraping/blob/main/Selenium/explicit_wait.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When should you use implicit and explicit waits?  \n",
    "Usually implicit waits are used when testing a script. They are short and simple, so it helps debuggind code faster.  \n",
    "On the other hand, explicit waits are long, but they help us write robust and efficient code so the driver doesn't have to waste time waiting a specific number of seconds, but continue with the execution as soon as the condition is satisfied.  \n",
    "To sum up, we can say that explicit wait are good to implement when the script is finished and we want to make that script more efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter Bot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check how to login to website using `Selenium` [here](https://github.com/sahaavi/Web-Scraping/blob/main/Selenium/twitter_login.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrape tweets from Twitter [check](https://github.com/sahaavi/Web-Scraping/blob/main/Selenium/scrape_tweets.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Twitter is a infinite scrolling site. Check how to scroll infinitely using `Selenium` [here](https://github.com/sahaavi/Web-Scraping/blob/main/Selenium/infinite_scrolling.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check how to scroll infinitely using `Selenium` and scrape tweet data with username, tweet, reply count, tweet id etc. [here](https://github.com/sahaavi/Web-Scraping/blob/main/Selenium/scrape_with_infinite_scrolling.py)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
