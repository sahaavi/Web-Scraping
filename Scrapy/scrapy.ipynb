{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrapy Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to install 2 libraries\n",
    "- scrapy\n",
    "    - `conda install -c conda-forge scrapy` or `pip install Scrapy`\n",
    "- protego\n",
    "    - `conda install -c conda-forge protego` or `pip install Protego`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the avaiable commands by executing `scrapy` command on your terminal.  \n",
    "How scrapy works on your computer you can check it by doing the benchmark test by executing this command `scrapy bench`.  \n",
    "Fetch a URL using the Scrapy downloader(will give you the html) `scrapy fetch http://google.com`  \n",
    "Generate new spider using pre-defined templates (everytime we want to create a spider to scrape a website we have to use this command)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start a project use this command in the terminal `scrapy startproject project_name`\n",
    "After executing this command a folder will be created with the given project name. Inside that folder there will be another folder according to the project name and a `scrapy.cfg` file which will help us run commands.  \n",
    "Inside the project_name folder you will see the `items.py`. It will provide the ability to better structure the data was scraped.  \n",
    "We also have `middlewares.py`, here we can plug custom functionality to process the responses and request.  \n",
    "The `pipelines.py` stores items which create in a database like `mongo` or `sql`.  \n",
    "The `settings.py` adds extra configuration to the project.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start a new spider we have write the command `scrapy genspider spider_name link`  \n",
    "For ex. `scrpay genspider worldometer www.worldometers.info/world-population/population-by-country`.   \n",
    "After executing this you'll get a message like this  \n",
    "```\n",
    "Created spider 'worldometer' using template 'basic' in module:\n",
    "  scrapy_spider.spiders.worldometer\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can find the spider file inside the `spiders` folder which has been created after executing the above command. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Scrapy there are two popular templates. The `scrapy.spider` and the `CrawlSpider` the scrapy.spider is the simplest spider.  \n",
    "It doesn't provide any special functionality, but we can customize this template to scrape the way we want.  \n",
    "\n",
    "On the other hand, that CrawlSpider is the most commonly used spider for crawling regular websites. It provides some mechanisms for following links by defining a set of rules.\n",
    "\n",
    "*Note that crawling is not the same as scraping a website.*\n",
    "\n",
    "A crawler usually brosius the World Wide Web for the purpose of web indexing. But Web scraping is more about extracting information from websites.\n",
    "So a crawl spider might not be the best suited for your web scraping project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now take a look at the scrapy spider template.  \n",
    "![spider_temp](../images/scrapy_spider_temp.png)  \n",
    "This has a default class with the name of the spider we created.\n",
    "`start_urls` is the variable that contain the url which spider will start from and the parse function to handle the response of a request. \n",
    "\n",
    "So in a scrapy, we use `response` to find elements. This is just like the driver in selenium or the soup in beautifulsoup. This response represents the response we get after we sent request to a website.   \n",
    "Now, unlike selenium, we can only find elements with XPATHS on scrapy. Scrapy doesn't have functions like fine element by ids or class names or tag names. But we can still find these elements writing an equivalent XPath.\n",
    "\n",
    "To find element with XPath on scrapy, we have to write -   \n",
    "`response.xpath('//tag[@AttributeName=\"Value\"]')`  \n",
    "Like selenium and beautifulsoup we can also find multiple elements using scrapy by using `getall()` -   \n",
    "`response.xpath('//tag[@AttributeName=\"Value\"]').get()`  \n",
    "`response.xpath('//tag[@AttributeName=\"Value\"]').getall()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrapy offers an inbuilt way of saving and storing data through the `yield` keyword.  \n",
    "\n",
    "Yield takes only one of the following data types:\n",
    "- Request (Scrapy object)\n",
    "- BaseItem (Scrapy object)\n",
    "- Dict\n",
    "- None\n",
    "\n",
    "This means that you can’t try passing it a string or integer, else you’ll get an error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrapy Shell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to enter into the scrapy shell run this command on terminal `scrapy shell`.\n",
    "Typically, Request objects are generated in the spiders and pass across the system until they reach the Downloader, which executes the request and returns a Response object which travels back to the spider that issued the request.\n",
    "Create and Send request to url - `r = scrapy.Request(url='https://www.worldometers.info/world-population/population-by-country/')`  \n",
    "Open spider and see the result of the request - `fetch(r)`  \n",
    "Check the HTML of the website - `response.body`  \n",
    "Specific elements - `response.xpath('//h1')`  \n",
    "Get the text of specific element - `response.xpath('//h1/text()')` By this we get the text but not clean still have the attribute names in it.  \n",
    "Get the clean text of specific element - `response.xpath('//h1/text()').get()`\n",
    "Get all the country names from the above website - `response.xpath('//td/a/text()').getall()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do the same thing by building a spider, check the scrapy spider template screenshot above. Code can be found [here](https://github.com/sahaavi/Web-Scraping/blob/main/Scrapy/scrapy_spider/scrapy_spider/spiders/worldometer.py).  \n",
    "To execute this spider run this command `scrapy crawl spider_name`. In this case the spider name is `worldometer`.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting links listed in a website [click](https://github.com/sahaavi/Web-Scraping/blob/main/Scrapy/scrapy_spider/scrapy_spider/spiders/worldometer_links.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping data from multiple links [here](https://github.com/sahaavi/Web-Scraping/blob/main/Scrapy/scrapy_spider/scrapy_spider/spiders/worldometer_multi_links.py).  \n",
    "To export the data into a json or csv file use this command `scrapy crawl spider_name -o filename.extension`.  \n",
    "For ex. `scrapy crawl worldometer_multi_links -o population.json`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pagination with Scrapy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a new spider for this. This time we'll use [audible website (Canada Region)](https://www.audible.ca/search)  \n",
    "Scapre the first page [code](https://github.com/sahaavi/Web-Scraping/blob/main/Scrapy/scrapy_spider/scrapy_spider/spiders/audible.py)  \n",
    "Scrape all pages [code](https://github.com/sahaavi/Web-Scraping/blob/main/Scrapy/scrapy_spider/scrapy_spider/spiders/audible_pagination.py)  \n",
    "*Like selenium and beautifulsoup we can't click on a button using Scrapy*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrapy is way faster than selenium and beautifulsoup. It scraped 25 pages, each contains info of approx 20 books in just 24 seconds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change User Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this command in the terminal to check the user agent `scrapy shell \"link\"`  \n",
    "For ex. `scrapy shell \"https://www.audible.ca/search\"` And now to get the user agent use `request.headers`. Then you'll get a dictionary. One of the key of this dictionary is User-Agent. The value of this user agent is Scrapy. That's how the website will easily know that we're using scrapy to scrape the website. Therefore, we have to change the user agent so it should look like we're sending request using chrome.  \n",
    "\n",
    "Let's check how to check the user agent in the browser -  \n",
    "Step 1: Inspect  \n",
    "![INSPECT](../images/inspect_audible.png)  \n",
    "Step 2: Select the Network option and reload the page and wait until all the elements are loaded.\n",
    "![Network](../images/network.png)\n",
    "Step 3: After loading the page search for any HTML request in the search bar.  \n",
    "![search html](../images/search.png)\n",
    "Step 4: Click any of the name you'll get the User-Agent on the right.  \n",
    "![user agent](../images/header.png)  \n",
    "\n",
    "Now that we have the value of user agent go to the `settings.py` file inside project folder that we created earlier.  \n",
    "Inside the `settings.py` file change the key value of `DEFAULT_REQUEST_HEADERS` dictionary like below -  \n",
    "```\n",
    "DEFAULT_REQUEST_HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36 Edg/114.0.1823.82'\n",
    "}\n",
    "```\n",
    "We can also do this in the actual spider file check [here](paste link) \n",
    "\n",
    "[Here](https://explore.whatismybrowser.com/useragents/explore/) you can find user agent for different platforms and softwares.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spider template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the available spider template using this command - `scrapy genspider -l`  \n",
    "To create a spider using `crawl` template use this command - `scrapy genspider -t crawl spider_name link`  \n",
    "For ex. `scrapy genspider -t crawl transcripts subslikescript.com/movies`   \n",
    "Now we'll have new property inside that spider file which is rule. The rule object tells spider what are the links we want to scrape while scraping.  \n",
    "`rules = (Rule(LinkExtractor(allow=r\"Items/\"), callback=\"parse_item\", follow=True),)`\n",
    "`LinkExtractor` will follow the links that contains the string mentioned in `allow` parameter and will use the `parse_item` function to parse information from those links. We can also use `deny` parameter instead of `allow` which will not follow the links mentioned in deny parameter. Another parameter is `restrict_xpaths` which restrict the spider to follow only the `xpaths expression` mentioned in this parameter.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrapy doesn't support `utf-8` encoding by default in earlier versions so we have to check in `settings.py` whether we have this line of code at the bottom or not.  \n",
    "`FEED_EXPORT_ENCODING = \"utf-8\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full code with crawler template [here](https://github.com/sahaavi/Web-Scraping/blob/main/Scrapy/scrapy_spider/scrapy_spider/spiders/transcripts_crawler.py)\n",
    "\n",
    "## Pagination with spider crawl template [code](https://github.com/sahaavi/Web-Scraping/blob/main/Scrapy/scrapy_spider/scrapy_spider/spiders/crawler_pagination.py)\n",
    "You have to set `DOWNLOAD_DELAY = 0.5` in `settings.py` otherwise you'll get 429 unknown status error which usually indicates that you are making too many requests to the server in a given time frame.   \n",
    "After running this you will see spider code is shorter and much faster than compare to soup and selenium "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change Crawler User Agent [code](https://github.com/sahaavi/Web-Scraping/blob/main/Scrapy/scrapy_spider/scrapy_spider/spiders/crawler_user_agent.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline\n",
    "We can modify the `Pipelines.py` file in the project folder in order to get message when the spider starts running and stops running.  \n",
    "By default there is only one function inside `Pipelines.py` which is `process_item`. We'll add two other funcitons - \n",
    " - open_spider\n",
    " - close_spider  \n",
    "Then we have to go the settings.py and uncomment these codes.  \n",
    "```\n",
    "ITEM_PIPELINES = {\n",
    "   \"scrapy_spider.pipelines.ScrapySpiderPipeline\": 300,\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll get messages like below whenever a spider opens because of the open spider function.  \n",
    " ```\n",
    "2023-07-18 11:03:56 [scrapy.core.engine] INFO: Spider opened\n",
    "2023-07-18 11:03:56 [crawler_user_agent] INFO: Spider opened: crawler_user_agent\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Data to MongoDB Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the account of [MongoDB](https://www.mongodb.com/) for free. Then create a project using free M0 option.\n",
    "Then install these packages in your pc.\n",
    "    - pymongo\n",
    "    - dnspython\n",
    "Use this command to install both together `conda install pymongo dnspython -y`  \n",
    "Then in network access of MongoDB project set this ip 0.0.0.0/0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then goto Database option in Deployment and select the Connect.  \n",
    "![mongo_connect](../images/mongo_connect.png)  \n",
    "Then choose the Driver option and select python and the your python version. Next copy the connection string which we have to paste inside the pipelines. \n",
    "Replace the `<password>` with the actual password you set.\n",
    "Then we have to change pipeline to MongoDBPipeline inside settings.py because this is the class name we used inside our Pipeline file.\n",
    "Check all the codes - \n",
    "[scrapy_mongo](paste folder link)\n",
    "    - [pipelines]()  \n",
    "    - [settings]()  \n",
    "    - [spider]()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then click on collections in Database on MongoDB to check the scraped data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Data SQLite Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find the codes for settings and pipelines in scrapy_mongo project under MongoDB Codes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
